package com.lee.spark

import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}
import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
import org.apache.spark.internal.Logging
import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage
import org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
import org.apache.spark.internal.io.HadoopMapReduceCommitProtocol
import org.apache.spark.mapred.SparkHadoopMapRedUtil

import scala.collection.mutable

class SmallFileMergeCommitProtocol(
                                    jobId: String,
                                    path: String,
                                    dynamicPartitionOverwrite: Boolean = false)
  extends SQLHadoopMapReduceCommitProtocol(jobId, path,dynamicPartitionOverwrite)
  with Serializable with Logging{

  @transient private var tempFiles: mutable.MutableList[String] = null


  override def setupJob(jobContext: JobContext): Unit = {
    logInfo("SmallFileMerge:: setupJob" + jobContext.getJobName + " Job ID: " + jobId + " Job output path:" + path)

    super.setupJob(jobContext)
  }

  override def setupTask(taskContext: TaskAttemptContext): Unit = {
    logInfo("SmallFileMerge:: setupTask" )
    tempFiles = mutable.MutableList[String]();
    super.setupTask(taskContext)
  }

  override def newTaskTempFile(
                                taskContext: TaskAttemptContext, dir: Option[String], ext: String): String = {
    val filename = getFilename(taskContext, ext)
    val tempFile = super.newTaskTempFile(taskContext,dir,ext)
    logInfo("SmallFileMerge:: newTaskTempFile " + filename + ". Ext is: " + ext + ". TempFile: " + tempFile)
    //TODO: record the files generated by the task
    tempFiles +=tempFile
    return tempFile
  }

  override def commitTask(taskContext: TaskAttemptContext): TaskCommitMessage = {
    logInfo("SmallFileMerge:: commitTask" )
    val attemptId = taskContext.getTaskAttemptID
    logInfo(s"Commit task ${attemptId}")
    tempFiles.foreach(f => println("commit file: " + f))
    val stageDir = System.getenv("SPARK_YARN_STAGING_DIR")
    logInfo("Stage dir: " + stageDir)
    //TODO: 在当前作业的hdfs:///user/spark/.sparkCompacting/jobId，生成task目录(如task_202203231931235379402080170456275_0000_m_000000)
    // 并根据获取的信息创建分区目录及空文件（这里需要注意的是，为了优化性能，这里的文件名是原始数据文件名和文件大小的拼接字符串）
    //

    super.commitTask(taskContext)
  }

  override def commitJob(jobContext: JobContext, taskCommits: Seq[TaskCommitMessage]): Unit = {
    logInfo("SmallFileMerge:: commitJob")
    //TODO: Merge the files
    // 搜索hdfs:///user/spark/.sparkCompacting/jobId目录来获得待写入数据文件的分区目录结构及文件名、文件大小等信息, 并合并文件
    super.commitJob(jobContext,taskCommits)
  }

}
